{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "import os\n",
    "import git\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n",
    "import hashlib\n",
    "from sklearn.utils import resample\n",
    "from matplotlib import pyplot\n",
    "import shutil\n",
    "import subprocess\n",
    "import appscript\n",
    "import time\n",
    "import researchpy as rp\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloning all repos into localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to where the names of the projects are stored\n",
    "csv_java_repos = pd.read_csv('path to csv file with porject names', sep='$')\n",
    "\n",
    "path  = \"path to cloning directory\" \n",
    "\n",
    "for index, row in csv_java_repos.iterrows():\n",
    "    clone = \"git clone https://github.com/\" + row.project + \".git\" \n",
    "    os.system(\"sshpass -p mm6pm18s9 ssh khaledwalidsabbagh\")\n",
    "    os.chdir(path) # Specifying the path where the cloned project needs to be copied\n",
    "    os.system(clone) # Cloning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = []\n",
    "for name in repo_names: #repo_names is a list that contains all project names\n",
    "    try:\n",
    "        repo = git.Repo(\"path to directory where all cloned repos exist\" + name ) \n",
    "        commits_list = list(repo.iter_commits())\n",
    "        commits_repo = []\n",
    "        data= []\n",
    "        for commit in commits_list: \n",
    "             commits_repo.append(commit.hexsha)\n",
    "        \n",
    "        #names of the metrics that we plan to extract\n",
    "        col_list = [\"git_diff_src_churn\", \"gh_diff_files_modified\",\n",
    "                    \"gh_sloc\", \"gh_num_commits_on_files_touched\", \"gh_num_commits_in_push\", \"tr_original_commit\",\n",
    "                    \"gh_team_size\", \"git_prev_commit_resolution_status\", \"gh_num_commit_comments\",\n",
    "                    \"git_num_all_built_commits\", \"gh_diff_files_added\", \"gh_diff_files_deleted\", \"gh_build_started_at\",\n",
    "                    \"gh_diff_src_files\", \"gh_diff_doc_files\", \"gh_diff_other_files\", \"tr_status\"]\n",
    "        \n",
    "        #loading data with the desired metrics only\n",
    "        cod_info = pd.read_csv('path to TravisTorrent data-set' + name+'.csv', sep=',', usecols=col_list)\n",
    "        cod_info = cod_info.sort_values(by ='gh_build_started_at', ascending=True)\n",
    "        \n",
    "        \n",
    "        count_not_found = 0\n",
    "        count_match = 0 \n",
    "        verdict = 0\n",
    "        \n",
    "        #now we define the format of the header that ccflex accepts\n",
    "        header = [\"id\",\"line\",\"contents\",\"class_name\",\"class_value\",\"path\",\"date\"] \n",
    "\n",
    "        #now we start by writing the header in a csv file\n",
    "        with open('path to where the csv should be written', 'a', newline='', encoding=\"utf-8\") as file:\n",
    "                                    writer = csv.writer(file, quoting=csv.QUOTE_ALL, delimiter='$')\n",
    "                                    writer.writerow([g for g in header])\n",
    "        \n",
    "        #now we start computing diff between pairs of hash commits and do some filtering of lines using RegExp\n",
    "        for count, commit_CIBench in enumerate(cod_info.tr_original_commit):\n",
    "                 if (str(commit_CIBench) in commits_repo and count<len(cod_info)):\n",
    "                    source_commit = repo.commit(str(commit_CIBench))\n",
    "                    if (count+ 1 < len(cod_info)):\n",
    "                        if (cod_info.tr_original_commit.loc[count+1] in commits_repo):\n",
    "                                target_commit = repo.commit(str(cod_info.tr_original_commit.loc[count+1]))\n",
    "                                git_diff= repo.git.diff(source_commit, target_commit)\n",
    "                                pattern_code = re.compile('([+].*\\n)|([+]\\s.*\\n)|([+++].*\\n)|([+][a-zA-Z]+.*)')\n",
    "                                code_result = re.findall(pattern_code, git_diff)\n",
    "                                code_to_str= ''.join(''.join(elems) for elems in code_result)\n",
    "                                if cod_info.tr_status[count] == \"passed\": \n",
    "                                      verdict = 1\n",
    "                                elif cod_info.tr_status[count] == \"failed\": \n",
    "                                      verdict = 0\n",
    "                                        \n",
    "                                writeDiff(code_to_str, verdict, cod_info.gh_build_started_at.loc[count+1], name)\n",
    "                                count_match = count_match + 1\n",
    "                        else:\n",
    "                            count_not_found = count_not_found + 1\n",
    "                    else: \n",
    "                        break\n",
    "        print (\"number of commits in travistorrent file: \" + str(count_match))\n",
    "        print (\"number of commits not found in repo: \" + str(count_not_found))\n",
    "        \n",
    "        #now we write the extracted diff of lines of code\n",
    "        _lines = pd.read_csv('projects/train-lines/diff_'+ name + '.csv', sep='$')\n",
    "        WriteBuildInformation(name, _lines, False)\n",
    "\n",
    "        df_majority = _lines[_lines.class_value == 1] \n",
    "        df_minority = _lines[_lines.class_value == 0]\n",
    "\n",
    "        #now we check the distribution of lines of code and see if we should resample the minority class\n",
    "        if BalancingType(df_majority, df_minority) == -1:\n",
    "            df_balanced = Balance_Classes(df_majority,df_minority, -1)\n",
    "        elif BalancingType(df_majority, df_minority) == 1:\n",
    "            df_balanced = Balance_Classes(df_majority,df_minority, 1)\n",
    "        else:\n",
    "            df_balanced = _lines\n",
    "        \n",
    "        #after resampling, we will write a new csv version of all extracted lines of code\n",
    "        df_balanced.to_csv('path to where you want to save the code changes in a csv file', sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "        \n",
    "        #now we need move the csv file of code changes into the ccflex directory. This is important as ccflex\n",
    "        #is programmed to read input files from a specific directory pyccflex/ccflex_tmp/processing/\n",
    "        \n",
    "        shutil.copy('path to csv file', 'path to ccflex directory/pyccflex/ccflex_tmp/processing/')\n",
    "        os.rename('path to csv file', 'path to ccflex directory/train-lines.csv')\n",
    "\n",
    "        WriteBuildInformation(name, df_balanced, False)\n",
    "        \n",
    "        # we now call ccflex to start generating feature vectors from the extracted code changes\n",
    "        Create_BoW(name)\n",
    "        _bow = pd.read_csv('train-bag-of-words-'+ name + '.csv', sep='$')\n",
    "        #  we now need to generate experimental subjects to run the experiment on\n",
    "        Stratify_Sample_BoW(_bow, name)\n",
    "        #after splitting the feature vectors into 10 folds, we start by training a model on every combination of \n",
    "        #9 folds\n",
    "        Learn_Build_Prediction(name, False)\n",
    "        \n",
    "        #now we repeat the same procedure for the build records without the need to generate feature vectors\n",
    "        #----------------------------------------------------------\n",
    "        df_majority = cod_info[cod_info.tr_status=='passed'] \n",
    "        df_minority = cod_info[cod_info.tr_status=='failed']\n",
    "        \n",
    "        if BalancingType(df_majority, df_minority) == -1:\n",
    "            df_balanced = Balance_Classes(df_majority,df_minority, -1)\n",
    "        elif BalancingType(df_majority, df_minority) == 1:\n",
    "            df_balanced = Balance_Classes(df_majority,df_minority, 1)\n",
    "        else:\n",
    "            df_balanced = cod_info\n",
    "        WriteBuildInformation(name, cod_info, True)\n",
    "        Stratify_Samples(df_balanced, 'tsm', name)\n",
    "        Learn_Build_Prediction(name, True)\n",
    "        \n",
    "    except:\n",
    "        print(\"An exception occurred in project: \" + name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteBuildInformation(projectname, _builds, isBuild):\n",
    "    if isBuild:\n",
    "        filepath = 'projects/buildinfo.csv'\n",
    "        build_header = [\"Project Name\", \"Total Builds\", \"Failing Builds\", \"Passing Builds\"]\n",
    "    else:\n",
    "        filepath = 'projects/lineinfo.csv'\n",
    "        build_header = [\"Project Name\", \"Total Analyzed Lines\", \"Failing Lines\", \"Passing Lines\"]\n",
    "\n",
    "        \n",
    "    with open(filepath, mode = 'a', newline='', encoding=\"utf-8\") as file:\n",
    "                writer = csv.DictWriter(file, quoting=csv.QUOTE_ALL, delimiter='$', fieldnames = build_header)\n",
    "                _builds = _builds.dropna()\n",
    "                if isBuild == True:\n",
    "                        \n",
    "                        writer.writerow({'Project Name': projectname, \n",
    "                                         'Total Builds': len(_builds), \n",
    "                                         'Failing Builds': len(_builds[_builds.tr_status=='failed']), \n",
    "                                         'Passing Builds': len(_builds[_builds.tr_status=='passed'])\n",
    "                                        }\n",
    "                                       )   \n",
    "                       \n",
    "                else: \n",
    "                        writer.writerow({'Project Name':  projectname, \n",
    "                                         'Total Analyzed Lines': len(_builds), \n",
    "                                         'Failing Lines': len(_builds[_builds.class_value==0]), \n",
    "                                         'Passing Lines': len(_builds[_builds.class_value==1])\n",
    "                                        }\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BalancingType(df_majority, df_minority):\n",
    "    if (len(df_majority.index) + len(df_minority.index)) != 0:\n",
    "        if (len(df_majority.index))/(len(df_majority.index) + len(df_minority.index)) > 0.6: \n",
    "            return -1\n",
    "\n",
    "        elif (len(df_minority.index))/(len(df_majority.index) + len(df_minority.index)) > 0.6:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_Classes(df_majority, df_minority, action): #an action of -1 means that the minority class should be upsampled\n",
    "    if action == -1:\n",
    "        df_minority_upsampled = resample(df_minority, \n",
    "                                         replace=True,     \n",
    "                                         n_samples=len(df_majority.index),\n",
    "                                         random_state=123)\n",
    "\n",
    "        print ('upsampled the minority class to ' + str(len(df_majority.index))) \n",
    "        return pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    elif action == 1: \n",
    "        df_majority_upsampled = resample(df_majority, \n",
    "                                         replace=True,     \n",
    "                                         n_samples=len(df_minority.index),\n",
    "                                         random_state=123)\n",
    "        print ('upsampled the minority class to ' + str(len(df_minority.index))) \n",
    "        return pd.concat([df_minority, df_majority_upsampled])\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_BoW(repo_name):\n",
    "    appscript.app('Terminal').do_script('cd path to ccflex script pyccflex/py-ccflex/; ./run2.sh') \n",
    "\n",
    "    #sleep for 5 seconds while ccflex completes the generation of feature vectors for one file\n",
    "    while not os.path.exists('/pyccflex/ccflex_tmp/processing/train-bag-of-words.csv'):\n",
    "        time.sleep(5)\n",
    "\n",
    "    #making sure that ccflex does not overwrite existing file by renaming the file that it will generate\n",
    "    if os.path.isfile('pyccflex/ccflex_tmp/processing/train-bag-of-words.csv'):\n",
    "        os.rename('pyccflex/ccflex_tmp/processing/train-bag-of-words.csv', 'pyccflex/ccflex_tmp/processing/train-bag-of-words-' + repo_name + '.csv')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stratify_Samples(data, typeDf, project):\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    \n",
    "    if typeDf == 'tsm':\n",
    "        data.loc[data.git_prev_commit_resolution_status == \"merge_found\", \"git_prev_commit_resolution_status\"] = \"0\"\n",
    "        data.loc[data.git_prev_commit_resolution_status == \"build_found\", \"git_prev_commit_resolution_status\"] = \"1\"\n",
    "        data.loc[data.git_prev_commit_resolution_status == \"no_previous_build\", \"git_prev_commit_resolution_status\"] = \"0\"\n",
    "        data.loc[data.tr_status == \"passed\", \"tr_status\"] = '1'\n",
    "        data.loc[data.tr_status == \"failed\", \"tr_status\"] = '0'\n",
    "        data = data.dropna()\n",
    "        skf.get_n_splits(data.iloc[:, :-2], data.tr_status)\n",
    "\n",
    "        #if directory does not exist, create it\n",
    "        if not os.path.isdir('subjects/'):\n",
    "                os.mkdir('subjects/')\n",
    "        \n",
    "        #creating directories for training and testing for both TSM and TF\n",
    "        os.mkdir('/subjects/' + project)\n",
    "        os.mkdir('/subjects/' + project+ '/tsm')\n",
    "        os.mkdir('/subjects/' + project+ '/tsm/training/')\n",
    "        os.mkdir('/subjects/' + project+ '/tsm/testing/')\n",
    "        os.mkdir('/subjects/' + project+ '/bow')\n",
    "        os.mkdir('/subjects/' + project+ '/bow/training/')\n",
    "        os.mkdir('/subjects/' + project+ '/bow/testing/')\n",
    "        \n",
    "        counter = 0\n",
    "        #splitting \n",
    "        for train_index, test_index in skf.split(data.iloc[:, :-2], data.tr_status):\n",
    "\n",
    "            X_train, X_test, y_train, y_test = data.iloc[train_index], data.iloc[test_index], data.tr_status.iloc[train_index], data.tr_status.iloc[test_index]\n",
    "\n",
    "            X_train.to_csv(r'subjects/' + project+ '/tsm/training/' + 'x_train_lines' + str(counter) + \".csv\", sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "            X_test.to_csv(r'subjects/' + project + '/tsm/testing/' + 'x_test_lines' + str(counter) + \".csv\", sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "            y_train.to_csv(r'subjects/' + project+ '/tsm/training/' + 'x_train_dep' + str(counter) + \".csv\", sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "            y_test.to_csv(r'subjects/' + project + '/tsm/testing/' + 'x_test_dep' + str(counter) + \".csv\", sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "\n",
    "            counter = counter + 1\n",
    "    elif typeDf == 'bow':      \n",
    "        data = data.dropna()\n",
    "        skf.get_n_splits(data, data.class_value)\n",
    "        counter = 0\n",
    "        for train_index, test_index in skf.split(data, data.class_value):\n",
    "            X_train, X_test, y_train, y_test = data.iloc[train_index], data.iloc[test_index], data.class_value.iloc[train_index], data.class_value.iloc[test_index]\n",
    "            X_train.to_csv(r'subjects/' + project+ '/bow/training/' + 'x_train_lines' + str(counter) + \".csv\", sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "            X_test.to_csv(r'subjects/' + project + '/bow/testing/' + 'x_test_lines' + str(counter) + \".csv\", sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "            y_train.to_csv(r'subjects/' + project+ '/bow/training/' + 'x_train_dep' + str(counter) + \".csv\", sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "            y_test.to_csv(r'subjects/' + project + '/bow/testing/' + 'x_test_dep' + str(counter) + \".csv\", sep='$', quotechar='\"', quoting=csv.QUOTE_ALL, index=False)\n",
    "            counter = counter + 1\n",
    "\n",
    "    print ('finished splitting majority')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Learn_Build_Prediction(repo_name, isBuild):\n",
    "    counter = 0\n",
    "    total_fscore = 0\n",
    "    total_mcc = 0\n",
    "    result_ = {}\n",
    "    result_all = {}\n",
    "    result_bow = {}\n",
    "    forest= RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "\n",
    "    while counter < 10:\n",
    "        if isBuild:\n",
    "            #filepath should be set to the path where the prediction results of the TSM validation will be written \n",
    "            filepath = '/projects/tsm_results.csv'\n",
    "            train_indep = pd.read_csv(r'/subjects/' + repo_name + '/tsm/training/x_train_lines' + str(counter) + '.csv', sep='$')\n",
    "            train_dep = pd.read_csv(r'/subjects/' + repo_name + '/tsm/training/x_train_dep' + str(counter) + '.csv', sep='$')\n",
    "            test_indep = pd.read_csv(r'/subjects/' + repo_name + '/tsm/testing/x_test_lines' + str(counter) + '.csv', sep='$')\n",
    "            test_dep = pd.read_csv(r'/subjects/' + repo_name + '/tsm/testing/x_test_dep' + str(counter) + '.csv', sep='$')\n",
    "            \n",
    "            with open (filepath, 'a') as outcsv:\n",
    "                writer = csv.writer(outcsv)\n",
    "                writer.writerow(['project', 'metric', 'fold', 'precision', 'recall', 'test_f1','test_mcc'])\n",
    "            for column in train_indep:\n",
    "                if is_numeric_dtype(train_indep[column]):\n",
    "                    forest.fit(train_indep[[column]], train_dep)   \n",
    "                    y_pred = forest.predict(test_indep[[column]]) \n",
    "\n",
    "                    result_['project'] = repo_name\n",
    "                    result_['metric'] =  column\n",
    "                    result_['fold'] =  counter\n",
    "                    result_['precision'] = precision_score(test_dep, y_pred)\n",
    "                    result_['recall'] = recall_score(test_dep, y_pred)\n",
    "\n",
    "                    result_['test_f1'] = f1_score(test_dep, y_pred)\n",
    "                    result_['test_mcc'] = matthews_corrcoef(test_dep, y_pred)\n",
    "\n",
    "                    total_prec = total_fscore + result_['precision']\n",
    "                    total_recall = total_fscore + result_['recall']\n",
    "\n",
    "                    total_fscore = total_fscore + result_['test_f1']\n",
    "                    total_mcc = total_mcc + result_['test_mcc']\n",
    "\n",
    "                    with open(filepath, 'a') as f:\n",
    "                        fieldnames = ['project', 'metric', 'fold', 'precision', 'recall', 'test_f1','test_mcc']\n",
    "                        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                        writer.writerow(result_)\n",
    "                else:\n",
    "                    continue\n",
    "            new_train_indep = train_indep.select_dtypes(include=numerics)\n",
    "            new_test_indep = test_indep.select_dtypes(include=numerics)\n",
    "\n",
    "            forest.fit(new_train_indep.loc[:, new_train_indep.columns != 'tr_status'], train_dep)   \n",
    "            y_pred = forest.predict(new_test_indep.loc[:, new_train_indep.columns != 'tr_status'])\n",
    "            result_all['project'] = repo_name\n",
    "            result_all['metric'] =  'all tsm'\n",
    "            result_all['fold'] =  counter\n",
    "            result_all['precision'] = precision_score(test_dep, y_pred)\n",
    "            result_all['recall'] = recall_score(test_dep, y_pred)\n",
    "\n",
    "            result_all['test_f1'] = f1_score(test_dep, y_pred)\n",
    "            result_all['test_mcc'] = matthews_corrcoef(test_dep, y_pred)\n",
    "\n",
    "            with open(filepath, 'a') as f:\n",
    "                        fieldnames = ['project', 'metric', 'fold', 'precision', 'recall', 'test_f1','test_mcc']\n",
    "                        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                        writer.writerow(result_all)\n",
    "                        \n",
    "        else:\n",
    "            #filepath should be set to the path where the prediction results of the TF validation will be written \n",
    "\n",
    "            filepath = 'projects/bow_results.csv'\n",
    "            train_indep = pd.read_csv(r'subjects/' + repo_name + '/bow/training/x_train_lines' + str(counter) + '.csv', sep='$')\n",
    "            train_dep = pd.read_csv(r'subjects/' + repo_name + '/bow/training/x_train_dep' + str(counter) + '.csv', sep='$')\n",
    "            test_indep = pd.read_csv(r'subjects/' + repo_name + '/bow/testing/x_test_lines' + str(counter) + '.csv', sep='$')\n",
    "            test_dep = pd.read_csv(r'subjects/' + repo_name + '/bow/testing/x_test_dep' + str(counter) + '.csv', sep='$')\n",
    "            \n",
    "            with open (filepath, 'a') as outcsv:\n",
    "                writer = csv.writer(outcsv)\n",
    "                writer.writerow(['project', 'metric', 'fold', 'precision', 'recall', 'test_f1','test_mcc'])\n",
    "                \n",
    "            new_train_indep = train_indep.select_dtypes(include=numerics)\n",
    "            new_test_indep = test_indep.select_dtypes(include=numerics)\n",
    "\n",
    "            forest.fit(new_train_indep.loc[:, new_train_indep.columns != 'class_value'], train_dep)   \n",
    "            y_pred = forest.predict(new_test_indep.loc[:, new_train_indep.columns != 'class_value'])\n",
    "            result_bow['project'] = repo_name\n",
    "            result_bow['metric'] =  'token counts'\n",
    "            result_bow['fold'] =  counter\n",
    "            result_bow['precision'] = precision_score(test_dep, y_pred)\n",
    "            result_bow['recall'] = recall_score(test_dep, y_pred)\n",
    "\n",
    "            result_bow['test_f1'] = f1_score(test_dep, y_pred)\n",
    "            result_bow['test_mcc'] = matthews_corrcoef(test_dep, y_pred)\n",
    "\n",
    "            with open(filepath, 'a') as f:\n",
    "                        fieldnames = ['project', 'metric', 'fold', 'precision', 'recall', 'test_f1','test_mcc']\n",
    "                        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                        writer.writerow(result_bow)\n",
    "\n",
    "        counter = counter + 1\n",
    "        print('counter: ' + str(counter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
